{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d66b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: work, patient, think, positive, tool, look, time, course, know, thing, example, good, come, use, like, need, information, lot, quickly, cool\n",
      "\n",
      "Topic 2: work, tool, relatively, annoy, difficult, open, expect, trigger, electronically, care, transfer, actually, realize, little, medical, update, technology, grateful, long, anymore\n",
      "\n",
      "Topic 3: new, lot, datum, monitor, transfer, need, good, far, big, look, medical, like, nursing, write, gain, efficient, department, relatively, set, certainly\n",
      "\n",
      "Topic 4: software, tool, time, save, hour, life, module, text, app, everyday, program, question, actually, ecg, use, answer, gain, expectation, good, finding\n",
      "\n",
      "Topic 5: know, look, answer, question, positive, ecg, try, function, effect, exist, search, breath, depend, course, say, beginning, test, ray, quick, write\n",
      "\n",
      "Topic 6: patient, ecg, come, positive, doctor, able, effect, point, available, immediately, far, surgery, tool, nice, write, round, nursing, care, medication, experience\n",
      "\n",
      "Topic 7: transfer, datum, new, answer, try, relatively, question, monitor, time, know, patient, service, save, effect, say, sign, vital, hospital, test, breath\n",
      "\n",
      "Topic 8: status, right, search, helpful, save, quickly, overview, management, look, patient, need, know, quick, blood, value, entire, paper, appreciate, click, time\n",
      "\n",
      "Topic 9: ecg, access, great, status, long, leave, home, quickly, early, text, software, know, clear, module, search, operation, hospital, surgery, fast, image\n",
      "\n",
      "Topic 10: documentation, care, appreciate, information, electronic, canton, fact, medication, search, point, round, think, moment, extreme, easy, long, emergency, open, ask, quickly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LSA Model, incl. stopwords and 20 words per topic\n",
    "\n",
    "# Question 1 - positive experience \n",
    "\n",
    "import docx\n",
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Specify the path to the folder containing all Word documents\n",
    "folder_path = \"/Users/mariewosny/Desktop/HSG/10_Conferences/2024_MIE/MIE2024docx/Quest1\"\n",
    "\n",
    "# List all files in the folder\n",
    "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".docx\")]\n",
    "\n",
    "# Preprocessing and stopwords for the English language\n",
    "processed_docs_all = []\n",
    "\n",
    "custom_stopwords = set(['oh', 'ah', 'okay'])\n",
    "\n",
    "for file_path in file_paths:\n",
    "    doc = docx.Document(file_path)\n",
    "    text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "    # Remove occurrences of the word \"okay\"\n",
    "    text_without_okay = ' '.join([word for word in text.split() if word.lower() != 'okay'])\n",
    "\n",
    "    processed_doc = ' '.join([token.lemma_ for token in nlp(text_without_okay) if not token.is_stop\n",
    "                              and token.is_alpha \n",
    "                              and token.lemma_ not in custom_stopwords])\n",
    "    processed_docs_all.append(processed_doc)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "tfidf_data = vectorizer.fit_transform(processed_docs_all)\n",
    "\n",
    "# Define the number of topics (or components in LSA)\n",
    "n_topics = 10\n",
    "\n",
    "# Create a Truncated SVD (LSA) model\n",
    "lsa = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "\n",
    "# Fit the model to the TF-IDF data\n",
    "lsa.fit(tfidf_data)\n",
    "\n",
    "# Transform the TF-IDF data using the fitted LSA model\n",
    "lsa_topic_matrix = lsa.transform(tfidf_data)\n",
    "\n",
    "# Number of top words per topic\n",
    "num_top_words = 20\n",
    "    \n",
    "# Print the top 20 words for each topic\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "for topic_idx, topic in enumerate(lsa.components_):\n",
    "    top_words_idx = topic.argsort()[:-num_top_words-1:-1]\n",
    "    top_words = feature_names[top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9347ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: work, patient, time, tool, think, program, example, thing, yes, like, problem, need, know, course, day, good, say, information, come, lot\n",
      "\n",
      "Topic 2: program, patient, information, lot, correct, look, medication, unnecessary, error, example, frustration, clinical, inefficient, interface, different, open, time, new, wrong, certain\n",
      "\n",
      "Topic 3: program, time, work, day, think, frustrating, correct, ticket, yes, pc, duty, long, new, night, frustration, cost, inefficient, lot, morning, application\n",
      "\n",
      "Topic 4: patient, paper, wait, switch, computer, insanely, ago, annoying, week, room, happen, lab, manage, disaster, large, crash, program, work, affect, resident\n",
      "\n",
      "Topic 5: write, time, change, page, day, medication, button, anesthesia, super, example, ticket, week, paper, space, negative, different, outage, prescription, long, enter\n",
      "\n",
      "Topic 6: patient, time, digital, open, actually, datum, solve, problem, different, house, source, place, ward, forget, file, insanely, able, app, start, relatively\n",
      "\n",
      "Topic 7: blood, paper, wait, lab, scan, plan, life, disaster, click, open, happen, measure, place, want, program, everyday, switch, lot, moment, datum\n",
      "\n",
      "Topic 8: blood, feeling, want, time, certain, need, patient, scan, uncertainty, intuitive, term, team, able, think, efficiently, annoying, application, treatment, event, emergency\n",
      "\n",
      "Topic 9: time, source, complicated, error, forget, life, open, example, term, access, clear, intuitive, work, measure, uncertainty, prescription, right, understand, phone, scan\n",
      "\n",
      "Topic 10: application, frustrating, blood, think, tool, scan, certain, house, board, mail, document, ward, enter, tumor, efficient, course, need, dictation, space, benefit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LSA Model, incl. stopwords and 20 words per topic\n",
    "\n",
    "# Question 2 - negative experience \n",
    "\n",
    "import docx\n",
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Specify the path to the folder containing all Word documents\n",
    "folder_path = \"/Users/mariewosny/Desktop/HSG/10_Conferences/2024_MIE/MIE2024docx/Quest2\"\n",
    "\n",
    "# List all files in the folder\n",
    "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".docx\")]\n",
    "\n",
    "# Preprocessing and stopwords for the English language\n",
    "processed_docs_all = []\n",
    "\n",
    "custom_stopwords = set(['oh', 'ah', 'okay'])\n",
    "\n",
    "for file_path in file_paths:\n",
    "    doc = docx.Document(file_path)\n",
    "    text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "    # Remove occurrences of the word \"okay\"\n",
    "    text_without_okay = ' '.join([word for word in text.split() if word.lower() != 'okay'])\n",
    "\n",
    "    processed_doc = ' '.join([token.lemma_ for token in nlp(text_without_okay) if not token.is_stop\n",
    "                              and token.is_alpha \n",
    "                              and token.lemma_ not in custom_stopwords])\n",
    "    processed_docs_all.append(processed_doc)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "tfidf_data = vectorizer.fit_transform(processed_docs_all)\n",
    "\n",
    "# Define the number of topics (or components in LSA)\n",
    "n_topics = 10\n",
    "\n",
    "# Create a Truncated SVD (LSA) model\n",
    "lsa = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "\n",
    "# Fit the model to the TF-IDF data\n",
    "lsa.fit(tfidf_data)\n",
    "\n",
    "# Transform the TF-IDF data using the fitted LSA model\n",
    "lsa_topic_matrix = lsa.transform(tfidf_data)\n",
    "\n",
    "# Number of top words per topic\n",
    "num_top_words = 20\n",
    "    \n",
    "# Print the top 20 words for each topic\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "for topic_idx, topic in enumerate(lsa.components_):\n",
    "    top_words_idx = topic.argsort()[:-num_top_words-1:-1]\n",
    "    top_words = feature_names[top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a875c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: patient, think, work, thing, change, doctor, actually, time, tool, lot, look, like, know, need, datum, simply, information, knowledge, use, course\n",
      "\n",
      "Topic 2: change, knowledge, know, tool, doctor, physician, today, person, long, increase, information, senior, past, medical, paper, able, maybe, situation, experience, mean\n",
      "\n",
      "Topic 3: actually, information, simply, like, use, datum, thing, help, offer, influence, quality, enjoy, new, hospital, machine, tool, depend, somewhat, exciting, example\n",
      "\n",
      "Topic 4: doctor, work, grow, change, pc, write, difference, job, ai, enjoy, thing, spend, problem, program, resident, professional, bit, important, open, time\n",
      "\n",
      "Topic 5: datum, think, simply, hand, profession, important, affect, development, assessment, digitization, colleague, improve, great, deal, accordingly, general, ai, offer, modern, damage\n",
      "\n",
      "Topic 6: like, document, feel, know, anesthesia, discipline, work, office, doctor, knowledge, think, medication, operate, program, play, computer, technical, colleague, compare, little\n",
      "\n",
      "Topic 7: ai, need, like, lot, information, money, important, use, new, datum, yes, form, standard, digitization, assessment, bring, say, tell, access, old\n",
      "\n",
      "Topic 8: ai, important, know, actually, change, like, knowledge, talk, computer, overall, thing, feel, definitely, half, grow, time, open, today, anymore, briefly\n",
      "\n",
      "Topic 9: hand, paper, datum, ultimately, past, help, access, read, pc, actually, screen, enjoy, lot, work, knowledge, profession, speak, physician, phone, clear\n",
      "\n",
      "Topic 10: half, consultation, difficult, big, world, money, grow, say, nursing, phone, situation, screen, hear, change, document, computer, conference, pandemic, standard, end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LSA Model, incl. stopwords and 20 words per topic\n",
    "\n",
    "# Question 3 - professional image \n",
    "\n",
    "import docx\n",
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Specify the path to the folder containing all Word documents\n",
    "folder_path = \"/Users/mariewosny/Desktop/HSG/10_Conferences/2024_MIE/MIE2024docx/Quest3\"\n",
    "\n",
    "# List all files in the folder\n",
    "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".docx\")]\n",
    "\n",
    "# Preprocessing and stopwords for the English language\n",
    "processed_docs_all = []\n",
    "\n",
    "custom_stopwords = set(['oh', 'ah', 'okay'])\n",
    "\n",
    "for file_path in file_paths:\n",
    "    doc = docx.Document(file_path)\n",
    "    text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "    # Remove occurrences of the word \"okay\"\n",
    "    text_without_okay = ' '.join([word for word in text.split() if word.lower() != 'okay'])\n",
    "\n",
    "    processed_doc = ' '.join([token.lemma_ for token in nlp(text_without_okay) if not token.is_stop\n",
    "                              and token.is_alpha \n",
    "                              and token.lemma_ not in custom_stopwords])\n",
    "    processed_docs_all.append(processed_doc)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "tfidf_data = vectorizer.fit_transform(processed_docs_all)\n",
    "\n",
    "# Define the number of topics (or components in LSA)\n",
    "n_topics = 10\n",
    "\n",
    "# Create a Truncated SVD (LSA) model\n",
    "lsa = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "\n",
    "# Fit the model to the TF-IDF data\n",
    "lsa.fit(tfidf_data)\n",
    "\n",
    "# Transform the TF-IDF data using the fitted LSA model\n",
    "lsa_topic_matrix = lsa.transform(tfidf_data)\n",
    "\n",
    "# Number of top words per topic\n",
    "num_top_words = 20\n",
    "    \n",
    "# Print the top 20 words for each topic\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "for topic_idx, topic in enumerate(lsa.components_):\n",
    "    top_words_idx = topic.argsort()[:-num_top_words-1:-1]\n",
    "    top_words = feature_names[top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d560ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
