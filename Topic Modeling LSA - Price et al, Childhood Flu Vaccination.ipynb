{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d66b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: feel, come, obviously, younger, school, guess, nhs, health, kid, interesting, young, speak, happen, ill, effect, letter, appointment, remember, cold, definitely\n",
      "\n",
      "Topic 2: wife, straightforward, pretty, faith, obviously, seek, letter, beneficial, medical, uk, website, alright, important, garden, world, scientist, certainly, science, personally, text\n",
      "\n",
      "Topic 3: son, team, specialist, blood, health, daughter, problem, visitor, today, advice, unwell, help, june, form, allergic, particularly, eat, play, laughter, necessarily\n",
      "\n",
      "Topic 4: daughter, feel, season, jab, mind, guess, issue, definitely, book, covid, friend, come, protect, vulnerable, mainly, strain, immunization, outbreak, importance, public\n",
      "\n",
      "Topic 5: son, specialist, team, unwell, phone, guess, faith, available, come, organize, clinic, arrange, word, medical, food, bear, horrible, reception, allergic, trust\n",
      "\n",
      "Topic 6: practice, guess, concerned, invite, try, rest, cost, fear, speak, provide, important, correct, ring, occur, mention, june, contact, winter, recover, frustrating\n",
      "\n",
      "Topic 7: blood, age, discuss, versus, guess, necessarily, cell, circumstance, particularly, understanding, community, wise, calpol, nurse, term, affect, body, red, cost, laughter\n",
      "\n",
      "Topic 8: honest, hour, practice, ago, course, appointment, unwell, injection, nursery, nasal, normally, week, school, experience, straight, wait, fine, issue, point, finish\n",
      "\n",
      "Topic 9: accept, expert, book, mind, daughter, suppose, immunize, visitor, chest, beneficial, practice, faith, small, important, night, especially, pneumonia, speak, let, immunization\n",
      "\n",
      "Topic 10: faith, beneficial, massive, stuff, ill, poorly, straight, somebody, trust, young, interesting, situation, base, check, medical, birth, bring, proper, parent, boy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LSA Model, incl. stopwords and 20 words per topic\n",
    "\n",
    "import docx\n",
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Specify the path to the folder containing all Word documents\n",
    "folder_path = \"/Users/mariewosny/Desktop/HSG/10_Conferences/2024_MIE/Dataset_flu\"\n",
    "\n",
    "# List all files in the folder\n",
    "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".docx\")]\n",
    "\n",
    "# Preprocessing and stopwords for the English language\n",
    "processed_docs_all = []\n",
    "\n",
    "custom_stopwords = set(['oh', 'ah'])\n",
    "\n",
    "for file_path in file_paths:\n",
    "    doc = docx.Document(file_path)\n",
    "    text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "    # Remove occurrences of the word \"okay\"\n",
    "    text_without_okay = ' '.join([word for word in text.split() if word.lower() != 'okay'])\n",
    "\n",
    "    processed_doc = ' '.join([token.lemma_ for token in nlp(text_without_okay) if not token.is_stop\n",
    "                              and token.is_alpha \n",
    "                              and token.lemma_ not in custom_stopwords])\n",
    "    processed_docs_all.append(processed_doc)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "tfidf_data = vectorizer.fit_transform(processed_docs_all)\n",
    "\n",
    "# Define the number of topics (or components in LSA)\n",
    "n_topics = 10\n",
    "\n",
    "# Create a Truncated SVD (LSA) model\n",
    "lsa = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "\n",
    "# Fit the model to the TF-IDF data\n",
    "lsa.fit(tfidf_data)\n",
    "\n",
    "# Transform the TF-IDF data using the fitted LSA model\n",
    "lsa_topic_matrix = lsa.transform(tfidf_data)\n",
    "\n",
    "# Number of top words per topic\n",
    "num_top_words = 20\n",
    "    \n",
    "# Print the top 20 words for each topic\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "for topic_idx, topic in enumerate(lsa.components_):\n",
    "    top_words_idx = topic.argsort()[:-num_top_words-1:-1]\n",
    "    top_words = feature_names[top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9347ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
