{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa70d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Model, incl. stopwords and 20 words per topic\n",
    "\n",
    "import docx\n",
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Specify the path to the folder containing all Word documents\n",
    "folder_path = \"/Users/mariewosny/HSG Python Projects/Input for Topic Modelling/English\"\n",
    "\n",
    "# List all files in the folder\n",
    "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".docx\")]\n",
    "\n",
    "# Preprocessing and stopwords for the English language\n",
    "processed_docs_all = []\n",
    "\n",
    "custom_stopwords = set(['probably', 'simply', 'exactly', 'bit', 'tell', 'okay', 'datum', 'stadt'])\n",
    "\n",
    "for file_path in file_paths:\n",
    "    doc = docx.Document(file_path)\n",
    "    text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "# Remove occurrences of the word \"okay\"\n",
    "    text_without_okay = ' '.join([word for word in text.split() if word.lower() != 'okay'])\n",
    "\n",
    "    processed_doc = ' '.join([token.lemma_ for token in nlp(text_without_okay) if not token.is_stop\n",
    "                              and token.is_alpha and token.lemma_ not in custom_stopwords])\n",
    "    processed_docs_all.append(processed_doc)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "data = vectorizer.fit_transform(processed_docs_all)\n",
    "\n",
    "# Define the number of topics\n",
    "\n",
    "n_topics = 10\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=500, random_state=42)\n",
    "\n",
    "# Fit the model to the data\n",
    "lda.fit(data)\n",
    "\n",
    "# Transform the data using the fitted model\n",
    "transformed = lda.transform(data)\n",
    "\n",
    "# Number of top words per topic\n",
    "num_top_words = 20\n",
    " \n",
    "# Print the top 20 words for each topic\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words_idx = topic.argsort()[:-num_top_words-1:-1]\n",
    "    top_words = feature_names[top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54d66b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: course, program, nursing, new, change, clinic, colleague, long, little, pms, people, important, doctor, write, physician, situation, problem, life, mean, app\n",
      "\n",
      "Topic 2: nursing, pms, project, employee, digitization, electronic, new, team, ward, chemotherapy, expert, electronically, important, involve, ai, process, staff, problem, care, management\n",
      "\n",
      "Topic 3: pms, emergency, anesthesia, ward, program, write, icu, nursing, intensive, click, ips, documentation, care, lab, document, page, medication, curve, transfer, triage\n",
      "\n",
      "Topic 4: pms, app, tumor, phone, surgery, video, emergency, cell, conference, board, home, test, cool, triage, electronically, personally, pc, write, cumbersome, recognition\n",
      "\n",
      "Topic 5: app, nursing, stadt, course, situation, kisim, ward, pdms, nurse, test, easy, document, technology, instrument, documentation, innovation, implementation, care, expert, chemotherapy\n",
      "\n",
      "Topic 6: radio, oncology, software, digitization, program, anesthesia, okay, transfer, paper, staff, solution, ips, technical, advantage, employee, face, new, error, stadt, hand\n",
      "\n",
      "Topic 7: crash, lab, allergy, clinic, request, function, write, ai, paper, laboratory, course, uster, okay, text, automatic, update, process, wait, drug, certainly\n",
      "\n",
      "Topic 8: kisim, radio, resident, software, face, okay, research, pc, ai, pdms, helpful, life, medicine, access, report, prepare, dragon, document, finding, emergency\n",
      "\n",
      "Topic 9: role, clinic, clinical, team, tumor, conference, practice, list, video, chatgpt, possible, project, external, people, retrieve, medication, report, page, feedback, helpful\n",
      "\n",
      "Topic 10: oncology, radio, chemotherapy, important, project, colleague, podcast, speak, board, app, dictate, chatgpt, ai, innovation, tumor, photo, dictation, translate, care, base\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LSA Model, incl. stopwords and 20 words per topic\n",
    "\n",
    "import docx\n",
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Specify the path to the folder containing all Word documents\n",
    "folder_path = \"/Users/mariewosny/HSG Python Projects/Input for Topic Modelling/English\"\n",
    "\n",
    "# List all files in the folder\n",
    "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".docx\")]\n",
    "\n",
    "# Preprocessing and stopwords for the English language\n",
    "processed_docs_all = []\n",
    "\n",
    "custom_stopwords = set(['probably', 'simply', 'exactly', 'bit', 'tell', 'okay', 'datum', 'stadt'])\n",
    "\n",
    "for file_path in file_paths:\n",
    "    doc = docx.Document(file_path)\n",
    "    text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "    # Remove occurrences of the word \"okay\"\n",
    "    text_without_okay = ' '.join([word for word in text.split() if word.lower() != 'okay'])\n",
    "\n",
    "    processed_doc = ' '.join([token.lemma_ for token in nlp(text_without_okay) if not token.is_stop\n",
    "                              and token.is_alpha and token.lemma_ not in custom_stopwords])\n",
    "    processed_docs_all.append(processed_doc)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "tfidf_data = vectorizer.fit_transform(processed_docs_all)\n",
    "\n",
    "# Define the number of topics (or components in LSA)\n",
    "n_topics = 10\n",
    "\n",
    "# Create a Truncated SVD (LSA) model\n",
    "lsa = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "\n",
    "# Fit the model to the TF-IDF data\n",
    "lsa.fit(tfidf_data)\n",
    "\n",
    "# Transform the TF-IDF data using the fitted LSA model\n",
    "lsa_topic_matrix = lsa.transform(tfidf_data)\n",
    "\n",
    "# Number of top words per topic\n",
    "num_top_words = 20\n",
    "    \n",
    "# Print the top 20 words for each topic\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "for topic_idx, topic in enumerate(lsa.components_):\n",
    "    top_words_idx = topic.argsort()[:-num_top_words-1:-1]\n",
    "    top_words = feature_names[top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a61e3f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: course, physician, people, long, laboratory, guideline, process, mean, change, certain, quality, management, electronic, today, canton, develop, problem, lab, medical, doctor\n",
      "\n",
      "Topic 2: pms, nursing, ward, emergency, new, project, employee, course, electronically, problem, documentation, electronic, document, important, little, staff, write, triage, performance, change\n",
      "\n",
      "Topic 3: program, anesthesia, intensive, clinical, icu, ward, care, medication, emergency, doctor, ecg, page, transfer, write, unit, blood, error, situation, read, heart\n",
      "\n",
      "Topic 4: tumor, program, phone, board, video, pms, conference, mail, colleague, podcast, cell, recognition, computer, dictate, report, list, surgery, send, cumbersome, speech\n",
      "\n",
      "Topic 5: app, phone, test, course, stadt, cell, surgery, colleague, speak, instrument, situation, translate, operation, technology, op, easy, room, prove, photo, short\n",
      "\n",
      "Topic 6: crash, lab, allergy, clinic, request, function, paper, okay, write, uster, wait, update, duty, text, course, clear, automatic, program, yeah, easy\n",
      "\n",
      "Topic 7: radio, oncology, new, medical, device, software, examination, solution, colleague, product, digitization, program, cloud, assessment, technical, employee, change, manufacturer, important, result\n",
      "\n",
      "Topic 8: kisim, life, okay, pc, honestly, dragon, software, pdms, face, research, medicine, hour, everyday, long, mean, little, general, option, phoenix, cool\n",
      "\n",
      "Topic 9: role, clinic, clinical, nursing, expert, course, difficult, practice, team, external, care, people, nurse, mean, end, group, change, process, possible, situation\n",
      "\n",
      "Topic 10: ai, chatgpt, project, retrieve, resident, diagnosis, nutritionist, base, important, possible, mention, report, hospitalization, feeling, polypoint, practice, clinic, medication, team, implement\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NMF Model, added stopwords and 20 words per topic\n",
    "\n",
    "import docx\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Specify the path to the folder containing all Word documents\n",
    "folder_path = \"/Users/mariewosny/HSG Python Projects/Input for Topic Modelling/English\"\n",
    "\n",
    "# List all files in the folder\n",
    "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".docx\")]\n",
    "\n",
    "# Preprocessing and stopwords for the English language\n",
    "processed_docs_all = []\n",
    "\n",
    "custom_stopwords = set(['probably', 'simply', 'exactly', 'bit', 'tell', 'okay', 'datum', 'stadt'])\n",
    "\n",
    "for file_path in file_paths:\n",
    "    doc = docx.Document(file_path)\n",
    "    text = ' '.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "    # Remove occurrences of the word \"okay\"\n",
    "    text_without_okay = ' '.join([word for word in text.split() if word.lower() != 'okay'])\n",
    "\n",
    "    processed_doc = ' '.join([token.lemma_ for token in nlp(text_without_okay) if not token.is_stop\n",
    "                              and token.is_alpha and token.lemma_ not in custom_stopwords])\n",
    "    processed_docs_all.append(processed_doc)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf_data = vectorizer.fit_transform(processed_docs_all)\n",
    "\n",
    "# Define the number of topics (components in NMF)\n",
    "n_topics = 10  # Adjust as needed\n",
    "# Create an NMF model\n",
    "nmf = NMF(n_components=n_topics, random_state=42)\n",
    "# Fit the model to the TF-IDF data\n",
    "nmf.fit(tfidf_data)\n",
    "\n",
    "# Number of top words per topic\n",
    "num_top_words = 20\n",
    "\n",
    "# Print the top 20 words for each topic\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    top_words_idx = topic.argsort()[:-num_top_words-1:-1]  # Adjust the number of top words as needed\n",
    "    top_words = feature_names[top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66baaff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
